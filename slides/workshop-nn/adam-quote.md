---
layout: quote
---

> # However, after a while people started noticing that despite superior training time, Adam in some areas does not converge to an optimal solution, so for some tasks (such as image classification on popular CIFAR datasets) state-of-the-art [results are still only achieved by applying SGD with momentum.](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c)

<div class="flex">
  <img alt="why" src="https://i.imgflip.com/68jy6a.jpg" style="height: 200px" />

  <p class="text-xl ml-8">
    <mdi-format-quote-open />More than that Wilson et. al. showed in their paper ‘The marginal value of adaptive gradient methods in machine learning’ that adaptive methods (such as Adam or Adadelta) do not generalize as well as SGD with momentum when tested on a diverse set of deep learning tasks, discouraging people to use popular optimization algorithms. A lot of research has been done since to analyze the poor generalization of Adam trying to get it to close the gap with SGD.<mdi-format-quote-close />
  </p>
</div>