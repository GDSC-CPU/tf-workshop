{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/psse-cpu/ml-workshop/blob/main/notebooks/workshop-templates/multivariate.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop, you'll predict the salary of an applicant, given the data of other\n",
    "applicants and the salary they're offered.\n",
    "\n",
    "- `column 1:` [Microsoft Python Certification Exam][1] score\n",
    "  * passing score is 28, so the range here is 28 to 40\n",
    "- `column 2:` years experience\n",
    "  * so far, we have applicants from 1 year to 12  years experience\n",
    "- `column 3:` monthly salary offered, $x100,000$ pesos\n",
    "\n",
    "> üëÄ large values like the salary need to be scaled down, otherwise your MSE might produce $\\infty$ costs.\n",
    "> Remember that before getting the average, we get the SUM first\n",
    "> and sum of squared errors of LARGE numbers like 150k salary can go towards $\\infty$  \n",
    ">  \n",
    "> You'll get a similar error with [house prices](https://www.kaggle.com/swathiachath/kc-housesales-data?select=kc_house_data.csv) in the hundred-thousand dollar range.\n",
    "\n",
    "[1]: https://www.udemy.com/course/microsoft-python-certification-exam-98-381-practice-tests/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from random import randrange\n",
    "\n",
    "# 3 decimal places, suppress scientific notation\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, data like these are loaded from CSV files, or databases, but that would require the participants\n",
    "to learn another library, with its own set of [invented syntax](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) that heavily uses\n",
    "[operator overloading](https://datapythonista.me/blog/python-operators-and-how-they-affect-pandas.html).  \n",
    "\n",
    "Ain't got no time for that in our 3 hour workshop.  So let's just use a hardcoded Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    [28.   ,  1.   ,  0.801],\n",
    "    [28.   ,  2.   ,  0.758],\n",
    "    [28.   ,  3.   ,  0.785],\n",
    "    [28.   ,  4.   ,  0.862],\n",
    "    [28.   ,  5.   ,  0.799],\n",
    "    [28.   ,  6.   ,  0.836],\n",
    "    [28.   ,  7.   ,  0.783],\n",
    "    [28.   ,  8.   ,  0.8  ],\n",
    "    [28.   ,  9.   ,  0.887],\n",
    "    [28.   , 10.   ,  0.904],\n",
    "    [28.   , 11.   ,  0.831],\n",
    "    [28.   , 12.   ,  0.848],\n",
    "    [29.   ,  1.   ,  0.839],\n",
    "    [29.   ,  2.   ,  0.836],\n",
    "    [29.   ,  3.   ,  0.803],\n",
    "    [29.   ,  4.   ,  0.83 ],\n",
    "    [29.   ,  5.   ,  0.877],\n",
    "    [29.   ,  6.   ,  0.884],\n",
    "    [29.   ,  7.   ,  0.871],\n",
    "    [29.   ,  8.   ,  0.888],\n",
    "    [29.   ,  9.   ,  0.915],\n",
    "    [29.   , 10.   ,  0.912],\n",
    "    [29.   , 11.   ,  0.859],\n",
    "    [29.   , 12.   ,  0.876],\n",
    "    [30.   ,  1.   ,  0.857],\n",
    "    [30.   ,  2.   ,  0.854],\n",
    "    [30.   ,  3.   ,  0.841],\n",
    "    [30.   ,  4.   ,  0.818],\n",
    "    [30.   ,  5.   ,  0.825],\n",
    "    [30.   ,  6.   ,  0.852],\n",
    "    [30.   ,  7.   ,  0.849],\n",
    "    [30.   ,  8.   ,  0.876],\n",
    "    [30.   ,  9.   ,  0.873],\n",
    "    [30.   , 10.   ,  0.88 ],\n",
    "    [30.   , 11.   ,  0.947],\n",
    "    [30.   , 12.   ,  0.964],\n",
    "    [31.   ,  1.   ,  0.825],\n",
    "    [31.   ,  2.   ,  0.912],\n",
    "    [31.   ,  3.   ,  0.909],\n",
    "    [31.   ,  4.   ,  0.856],\n",
    "    [31.   ,  5.   ,  0.893],\n",
    "    [31.   ,  6.   ,  0.91 ],\n",
    "    [31.   ,  7.   ,  0.867],\n",
    "    [31.   ,  8.   ,  0.974],\n",
    "    [31.   ,  9.   ,  0.921],\n",
    "    [31.   , 10.   ,  0.888],\n",
    "    [31.   , 11.   ,  0.905],\n",
    "    [31.   , 12.   ,  0.952],\n",
    "    [32.   ,  1.   ,  0.943],\n",
    "    [32.   ,  2.   ,  0.88 ],\n",
    "    [32.   ,  3.   ,  0.877],\n",
    "    [32.   ,  4.   ,  0.904],\n",
    "    [32.   ,  5.   ,  0.891],\n",
    "    [32.   ,  6.   ,  0.898],\n",
    "    [32.   ,  7.   ,  0.965],\n",
    "    [32.   ,  8.   ,  0.962],\n",
    "    [32.   ,  9.   ,  0.989],\n",
    "    [32.   , 10.   ,  0.976],\n",
    "    [32.   , 11.   ,  1.003],\n",
    "    [32.   , 12.   ,  1.02 ],\n",
    "    [33.   ,  1.   ,  0.891],\n",
    "    [33.   ,  2.   ,  0.888],\n",
    "    [33.   ,  3.   ,  0.975],\n",
    "    [33.   ,  4.   ,  0.932],\n",
    "    [33.   ,  5.   ,  0.929],\n",
    "    [33.   ,  6.   ,  0.976],\n",
    "    [33.   ,  7.   ,  0.983],\n",
    "    [33.   ,  8.   ,  0.97 ],\n",
    "    [33.   ,  9.   ,  0.957],\n",
    "    [33.   , 10.   ,  1.014],\n",
    "    [33.   , 11.   ,  0.981],\n",
    "    [33.   , 12.   ,  1.048],\n",
    "    [34.   ,  1.   ,  0.929],\n",
    "    [34.   ,  2.   ,  0.986],\n",
    "    [34.   ,  3.   ,  0.943],\n",
    "    [34.   ,  4.   ,  0.93 ],\n",
    "    [34.   ,  5.   ,  0.967],\n",
    "    [34.   ,  6.   ,  0.974],\n",
    "    [34.   ,  7.   ,  1.021],\n",
    "    [34.   ,  8.   ,  0.978],\n",
    "    [34.   ,  9.   ,  1.045],\n",
    "    [34.   , 10.   ,  0.972],\n",
    "    [34.   , 11.   ,  0.979],\n",
    "    [34.   , 12.   ,  0.986],\n",
    "    [35.   ,  1.   ,  1.027],\n",
    "    [35.   ,  2.   ,  0.964],\n",
    "    [35.   ,  3.   ,  1.041],\n",
    "    [35.   ,  4.   ,  1.048],\n",
    "    [35.   ,  5.   ,  1.065],\n",
    "    [35.   ,  6.   ,  0.972],\n",
    "    [35.   ,  7.   ,  0.979],\n",
    "    [35.   ,  8.   ,  1.036],\n",
    "    [35.   ,  9.   ,  0.993],\n",
    "    [35.   , 10.   ,  1.07 ],\n",
    "    [35.   , 11.   ,  1.017],\n",
    "    [35.   , 12.   ,  1.014],\n",
    "    [36.   ,  1.   ,  1.055],\n",
    "    [36.   ,  2.   ,  1.052],\n",
    "    [36.   ,  3.   ,  1.059],\n",
    "    [36.   ,  4.   ,  0.986],\n",
    "    [36.   ,  5.   ,  1.023],\n",
    "    [36.   ,  6.   ,  1.05 ],\n",
    "    [36.   ,  7.   ,  1.047],\n",
    "    [36.   ,  8.   ,  1.114],\n",
    "    [36.   ,  9.   ,  1.081],\n",
    "    [36.   , 10.   ,  1.088],\n",
    "    [36.   , 11.   ,  1.045],\n",
    "    [36.   , 12.   ,  1.112],\n",
    "    [37.   ,  1.   ,  1.003],\n",
    "    [37.   ,  2.   ,  1.1  ],\n",
    "    [37.   ,  3.   ,  1.077],\n",
    "    [37.   ,  4.   ,  1.054],\n",
    "    [37.   ,  5.   ,  1.021],\n",
    "    [37.   ,  6.   ,  1.048],\n",
    "    [37.   ,  7.   ,  1.055],\n",
    "    [37.   ,  8.   ,  1.112],\n",
    "    [37.   ,  9.   ,  1.109],\n",
    "    [37.   , 10.   ,  1.136],\n",
    "    [37.   , 11.   ,  1.153],\n",
    "    [37.   , 12.   ,  1.16 ],\n",
    "    [38.   ,  1.   ,  1.061],\n",
    "    [38.   ,  2.   ,  1.128],\n",
    "    [38.   ,  3.   ,  1.135],\n",
    "    [38.   ,  4.   ,  1.042],\n",
    "    [38.   ,  5.   ,  1.079],\n",
    "    [38.   ,  6.   ,  1.056],\n",
    "    [38.   ,  7.   ,  1.133],\n",
    "    [38.   ,  8.   ,  1.08 ],\n",
    "    [38.   ,  9.   ,  1.087],\n",
    "    [38.   , 10.   ,  1.184],\n",
    "    [38.   , 11.   ,  1.191],\n",
    "    [38.   , 12.   ,  1.178],\n",
    "    [39.   ,  1.   ,  1.049],\n",
    "    [39.   ,  2.   ,  1.056],\n",
    "    [39.   ,  3.   ,  1.133],\n",
    "    [39.   ,  4.   ,  1.15 ],\n",
    "    [39.   ,  5.   ,  1.127],\n",
    "    [39.   ,  6.   ,  1.134],\n",
    "    [39.   ,  7.   ,  1.101],\n",
    "    [39.   ,  8.   ,  1.188],\n",
    "    [39.   ,  9.   ,  1.145],\n",
    "    [39.   , 10.   ,  1.122],\n",
    "    [39.   , 11.   ,  1.219],\n",
    "    [39.   , 12.   ,  1.206],\n",
    "    [40.   ,  1.   ,  1.107],\n",
    "    [40.   ,  2.   ,  1.134],\n",
    "    [40.   ,  3.   ,  1.161],\n",
    "    [40.   ,  4.   ,  1.108],\n",
    "    [40.   ,  5.   ,  1.135],\n",
    "    [40.   ,  6.   ,  1.182],\n",
    "    [40.   ,  7.   ,  1.179],\n",
    "    [40.   ,  8.   ,  1.226],\n",
    "    [40.   ,  9.   ,  1.143],\n",
    "    [40.   , 10.   ,  1.18 ],\n",
    "    [40.   , 11.   ,  1.237],\n",
    "    [40.   , 12.   ,  1.194]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying we sliced $X$ correctly, just showing the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[28.,  1.],\n",
       "       [28.,  2.],\n",
       "       [28.,  3.],\n",
       "       [28.,  4.],\n",
       "       [28.,  5.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data[:, 0:2]\n",
    "X[0: 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying we sliced $y$ correctly, just showing the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.801],\n",
       "       [0.758],\n",
       "       [0.785],\n",
       "       [0.862],\n",
       "       [0.799]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data[:, [-1]]\n",
    "y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the lowest and highest of each of the following:\n",
    "- certification score\n",
    "- years experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28.,  1.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first the lowest of each\n",
    "np.min(X, axis=0) # 0 = x-axis, 1 = y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40., 12.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then the highest of each\n",
    "np.max(X, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we scale using Scikit-learn's scaler, which is easier.  We can also use numpy's vectorization, that's one less library to learn.\n",
    "\n",
    "Note that:\n",
    "- for certification scores of `40`, they become `1`s, `28` become `0`s.\n",
    "- for years experience of `1`, they become `0`s, `12` becomes `1`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X[-14:-9]` will show a few of those who got scores of 39 (veterans), and 40 (few years experience)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39., 11.],\n",
       "       [39., 12.],\n",
       "       [40.,  1.],\n",
       "       [40.,  2.],\n",
       "       [40.,  3.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm = X # <YOUR CODE TO FEATURE-SCALE X HERE>\n",
    "\n",
    "# Showing the original values\n",
    "X[-14:-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39., 11.],\n",
       "       [39., 12.],\n",
       "       [40.,  1.],\n",
       "       [40.,  2.],\n",
       "       [40.,  3.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the normalized ones\n",
    "X_norm[-14:-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COMPILE AND TRAIN YOUR MODEL HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[28.   ,  1.   ,  0.801],\n",
       "       [28.   ,  2.   ,  0.758],\n",
       "       [28.   ,  3.   ,  0.785],\n",
       "       [28.   ,  4.   ,  0.862],\n",
       "       [28.   ,  5.   ,  0.799],\n",
       "       [28.   ,  6.   ,  0.836],\n",
       "       [28.   ,  7.   ,  0.783],\n",
       "       [28.   ,  8.   ,  0.8  ],\n",
       "       [28.   ,  9.   ,  0.887],\n",
       "       [28.   , 10.   ,  0.904],\n",
       "       [28.   , 11.   ,  0.831],\n",
       "       [28.   , 12.   ,  0.848],\n",
       "       [29.   ,  1.   ,  0.839],\n",
       "       [40.   ,  9.   ,  1.143],\n",
       "       [40.   , 10.   ,  1.18 ],\n",
       "       [40.   , 11.   ,  1.237]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing our training data again, so we can compare\n",
    "# `vstack` -> stack two matrices vertically\n",
    "np.vstack((data[0:13], data[-4:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much will we offer?\n",
    "- another candidate barely passing, with 12 years exp\n",
    "- another _\"kabit\"_, but a 15 year veteran?\n",
    "  * we never had a 15 year veteran apply before\n",
    "- barely passing also, and with only half a year experience?\n",
    "  * never had an almost fresh-grad so far\n",
    "- perfect score, 15 years experience\n",
    "- perfect score, 12 years experience, like the guy offered $119.4k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the inputs for prediction as well, before passing it to `model.predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[28. , 12. ],\n",
       "       [28. , 15. ],\n",
       "       [28. ,  0.5],\n",
       "       [40. , 15. ],\n",
       "       [40. , 12. ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_input = np.array([ # THIS IS NOT SCALED YET, ALSO SCALE INPUT FOR PREDICTION\n",
    "    [28, 12], \n",
    "    [28, 15],\n",
    "    [28, 0.5],\n",
    "    [40, 15],\n",
    "    [40, 12]\n",
    "])\n",
    "\n",
    "\"\"\"\n",
    "If you wrote your code correctly, the scaled values should be:\n",
    "array([[ 0.   ,  1.   ],\n",
    "       [ 0.   ,  1.273],\n",
    "       [ 0.   , -0.045], # the -0.045 is because 0.5 is lower than the lowest years exp earlier\n",
    "       [ 1.   ,  1.273], # same with 15 years become 1.273\n",
    "       [ 1.   ,  1.   ]])\n",
    "\"\"\"\n",
    "\n",
    "scaled_input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <YOUR CODE TO PREDICT HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get values somewhere around:\n",
    "\n",
    "- 86,130.559\n",
    "- 89,092.261\n",
    "- 74,777.365\n",
    "- 126,128.817\n",
    "- 123,167.109\n",
    "\n",
    "üòâ This dataset was artificially generated, with one entry per _\"combo\"_ of\n",
    "score and years_exp, using the formula:\n",
    "\n",
    "80% * certification_score +  \n",
    "20% * years_experience +  \n",
    "$\\pm$ (1000, 2000, 3000, 4000, 5000 Php) (random)\n",
    "\n",
    "\n",
    "The weights should be somewhere around that ratio.  I got\n",
    "$$\n",
    "  w = \\begin{bmatrix}\n",
    "    0.753 \\\\\n",
    "    0.37 \\\\\n",
    "    0.109\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "A 77:23 ratio, pretty close to 80:20 üòÅ\n",
    "\n",
    "Using the weights to predict, when given an input, say `score = 40`, `years_exp = 15`, it would do:\n",
    "\n",
    "1. Feature Scaled 40 is `1`, feature scaled 15 is `1.273`\n",
    "2. $\\hat{y} = 0.109 \\cdot 1.273 + .37 \\cdot 1 + 0.753$  \n",
    "3. $\\hat{y} = 1.261757$\n",
    "\n",
    "but it's not yet $x100k$, so the predicted value is $126,175.7$ Php (actually 126,128.817 when our code\n",
    "is ran).\n",
    "\n",
    "The slightly bigger value from our _\"mano-mano\"_ computation is because all values are rounded off \n",
    "to 3 decimal places, but internally in our code, they're not.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2152fd7f0bbc62aa1baff8c990435d1e2c7175d001561303988032604c11a48"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
